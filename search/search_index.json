{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Modeling","text":"<p>A Data modeling document for any business data store.</p> <p>Note</p> <p>This project will focus on the data modeling knowledge and implementation.</p> <p>The end-to-end flow before implementation step.</p> <pre><code>---\ntitle: E2E Flow\n---\nstateDiagram-v2\n    direction LR\n    R: Requirement Gathering\n    D: Datasource Exploring\n    T: Transform Spec\n\n    [*] --&gt; R\n\n    R --&gt; D\n    D --&gt; R: recheck logic\n    D --&gt; T\n    T --&gt; [*]</code></pre>"},{"location":"#metadata","title":"Metadata","text":""},{"location":"#domain","title":"Domain","text":""},{"location":"#transform-spec","title":"Transform Spec","text":"<p>The table structure for keeping transformation spec for any transform that use on the ETL/ELT process.</p>"},{"location":"#control-framework","title":"Control Framework","text":""},{"location":"requirement-gathering/","title":"Requirement Gathering","text":"<p>One of the mistakes you\u2019ll make as a Data Engineer is not truly understanding the Business Requirements.</p> <p>Example</p> <p>Quote</p> <p>The business will come to you and ask for a real-time dashboard.<sup>1</sup></p> <p>But they mean they want the data updated 3-4x a day, or maybe they only look at the report once a week; at that moment, the data should be as up-to-date as possible.</p>"},{"location":"requirement-gathering/#getting-started","title":"Getting Started","text":""},{"location":"requirement-gathering/#identify-the-end-users","title":"Identify the End-Users","text":"<p> Begin by identifying the end-users, crucial stakeholders who utilize the project's output (Understanding the capabilities and preferences of the end-user is crucial for designing an appropriate solution).</p> <p>End-users (&amp; their preferences) for data projects are usually one of;</p> <ul> <li> <p>Data Analysts/Data Scientists</p> <p>SQL, No-SQL, CSV files</p> </li> <li> <p>Business Users</p> <p>Dashboards, Reports, Excel files</p> </li> <li> <p>Software Engineers</p> <p>SQL, APIs, CRMs, JSON files</p> </li> <li> <p>External Clients</p> <p>Cloud storage, SFTP/FTP, APIs, SQL</p> </li> </ul> <p>Warning</p> <p>If you do not know who is the end users, you can ask the Solution Architect in that data project.</p>"},{"location":"requirement-gathering/#help-end-users-define-requirements","title":"Help End-Users Define Requirements","text":"<p>Note</p> <p>Understand The Business - Not Just The Technical Requirements</p> <p> Assist end-users in defining requirements by engaging in conversations about their objectives and challenges. Understand their current operations to gain valuable insights.</p> <p>Use the following questions to refine requirements:</p> <ul> <li> <p>Business Impact</p> <p>Evaluate how the data impacts the business and quantify the improvements.</p> <ul> <li>How does having this data impact the business?</li> <li>What is the measurable improvement in the bottom line, business OKR, etc?   Knowing the business impact helps in determining if this project is worth   doing.</li> </ul> </li> <li> <p>Semantic Understanding</p> <ul> <li>What does the data represent? What business process generates this data?   Knowing this will help you model the data and understand its relation to other   tables in your warehouse.</li> </ul> <p>Grasp the data's representation and its relation to other warehouse tables.</p> </li> <li> <p>Data Source</p> <p>Where does the data originate? (an application database, external vendor via SFTP/Cloud store dumps, API data pull, manual upload, etc).</p> </li> <li> <p>Frequency of Data Pipeline</p> <ul> <li>How fresh does the data need to be? (n minutes, hourly, daily, weekly, etc).</li> <li>Is there a business case for not allowing a higher frequency?</li> <li>What is the highest frequency of data load acceptable by end-users?</li> </ul> </li> <li> <p>Data Output Requirements</p> <ul> <li>What is the data output schema? (table name, column names, API field names,   Cloud storage file name/size, etc)</li> </ul> </li> <li> <p>Historical Data</p> <ul> <li>Does historical data need to be stored? When loading data into a warehouse,   the answer is usually yes.</li> </ul> </li> <li> <p>Data Caveats</p> <ul> <li>Does the data have any caveats? (e.g. seasonality affecting size, data skew,   inability to join, or data unavailability).</li> <li>Are there any known issues with upstream data sources, such as late arriving   data, or missing data?</li> </ul> </li> <li> <p>Access Pattern</p> <ul> <li>How will the end user access the data? Is access via SQL, dashboard tool,   APIs, or cloud storage? In the case of SQL or dashboard access, What are the   commonly used filter columns (e.g. date, some business entity)?</li> <li>What is the expected access latency?</li> </ul> </li> <li> <p>Business Rules Check (QA)</p> <ul> <li>What data quality metrics do the end-users care about?</li> <li>What are business logic-based data quality checks? Which numeric fields   should be checked for divergence (e.g. can\u2019t differ by more than x%) across   data pulls?</li> </ul> </li> </ul> <p> Show appreciation to end-users for their time, keep them updated on progress, incorporate their feedback, suggest solutions for common issues, and acknowledge their expertise when presenting the project to a wider audience.</p> <p>Clearly define the requirements, record them (e.g. JIRA, etc), and get sign-off from the stakeholders.</p>"},{"location":"requirement-gathering/#end-user-validation","title":"End-User Validation","text":"<p> Provide end-users with sample data for analysis, allowing them to validate its accuracy and usability. Record any new requirements or changes, getting sign-off from stakeholders before proceeding.</p> <p>Record any new requirements or changes (e.g. JIRA, etc), and get sign-off from the stakeholders. Do not start work on the transformation logic until you get a sign-off from the stakeholders.</p>"},{"location":"requirement-gathering/#deliver-iteratively","title":"Deliver Iteratively","text":"<p> Break down large projects into smaller, manageable parts and work with stakeholders to set timelines and priorities. This approach facilitates a short feedback cycle, making it easier to adapt to changing requirements. Track progress with clear acceptance criteria.</p> <p>Example</p> <p>If you are building an ELT pipeline (REST API =&gt; dashboard), you can split it into modeling the data, pulling data from a REST API, loading it into a raw warehouse table, &amp; building a dashboard for the modeled data.</p> <p>Delivering in small chunks enables a short feedback cycle from the end-user making changing requirements easy to handle. Track your work (tickets, etc) with clear acceptance criteria.</p>"},{"location":"requirement-gathering/#handling-changing-requirementsnew-features","title":"Handling Changing Requirements/New Features","text":"<p> Establish a process for handling change or feature requests, ensuring end-users can request modifications. Prioritize requests with stakeholder input, communicate delivery timelines, and educate end-users on the request process to prevent scope creep and maintain timely delivery.</p> <p>Warning</p> <p>Do not accept Adhoc change/feature requests! (unless it\u2019s an emergency).</p> <p>Create a process to ...</p> <ul> <li>Allow end-users to request changes/features</li> <li>Prioritize the change/feature requests with help from stakeholders</li> <li>Decide and communicate delivery timelines to end-users</li> </ul> <p>Educate the end-user on the process of requesting a new feature/change. Following a process will prevent scope creep and allow you to deliver on time.</p>"},{"location":"requirement-gathering/#examples","title":"Examples","text":""},{"location":"requirement-gathering/#project-starter","title":"Project Starter","text":"Question Answer What responsibility of a data engineer on this project? Load all data sources from RDBMS to the warehouse for making dashboards by DA."},{"location":"requirement-gathering/#data-component","title":"Data Component","text":"Question Answer What is the data source system type? RDBMS (Postgres, MySQL, SQL Server, etc.),NO-SQL (MongoDB, Elastic),API What is data compute service to extract from the source system? Docker Application (FastAPI), Batch job via Cloud services (Azure Batch, AWS Batch, etc.) ETL Services (Azure Databricks, AWS Glue etc.) How to authenticate to the data source system? Use user and password. Use service account on that cloud provider."},{"location":"requirement-gathering/#data-source","title":"Data Source","text":"<p>These questions use per data source.</p> Question Answer What is data source name? What is the schema of this source (column and data type docs)? What is the primary of this data source? How to ingest data to this data source? How to incremental load of this data source? The filter fields for filter period of changed data like create_date, updated_date etc. What is loading type of this data source? Incremental via Merge or Append.Full-dump load. When to load this data source that does not effect to the source system? Every 1:30 AM to 3:00AM is good or not? What is the rule check for this data?"},{"location":"requirement-gathering/#conclusion","title":"Conclusion","text":"<p>Effectively managing ever-changing requirements is a challenging aspect of a data engineer's role. By following the steps outlined in this article, you can navigate these challenges, ensuring timely project delivery, making a significant impact, enjoying your work on data projects, and fostering supportive end-users.</p> <p>The next time you start a data project, follow the steps shown above to</p> <ul> <li>Deliver on Time</li> <li>Make a Huge Impact</li> <li>Make working on the data projects a Joy</li> <li>Build supportive end-users</li> </ul> <ol> <li> <p>Becoming a Better Data Engineer Tips \u21a9</p> </li> <li> <p>How to gather requirements for your data project \u21a9</p> </li> </ol>"},{"location":"transform-spec/","title":"Transform Spec","text":"<p>When the requirement of data ingestion and transformation was completed by user, the final product that use to communicate is the transformation spec data.</p>"},{"location":"transform-spec/#metadata-of-transform-spec","title":"Metadata of Transform Spec","text":"Component Column Name Description Data Type PDM Information (Semantic Layer) Database Database or schema name STRING Table Name / File Name Table or file name STRING Column Name Column name STRING Data Type Data type STRING Key Flag for primary key marking BOOLEAN Field Definition Definition of this field STRING Source Information Source System / Database STRING Table Name / File Name STRING Alias STRING Column STRING Transform Business Rule Type LITERAL[\"Not Mapped\", \"Constant\", \"Not Mapped\", \"Move\", \"Filter\", \"Join\"] Business Rule / Join / Condition STRING Remark STRING Updating Information Updated Date DATETIME Updated By STRING Remark STRING <p>Example</p> <p>If the transformation spec fron the user that provide to you be like;</p> <pre><code>WITH addr AS (\n    SELECT DISTINCT\n        cust_address\n        , sub_district_code\n        , district_code\n        , province_code\n        , post_code\n    FROM `data-prod-mobile.b2c_customer.transaction`\n)\nSELECT\n      md5(concat(\n        coalesce(cust_address, '')\n        , coalesce(sub_district_code, '')\n        , coalesce(district_code, '')\n        , coalesce(province_code, '')\n        , coalesce(post_code, '')\n      ))                                                    AS HOUSE_ID\n    , 1                                                     AS HOUSE_LVL\n    , cust_address                                          AS ADDR\n    , null                                                  AS HOUSE_NO\n    , null                                                  AS STREET\n    , sub_district_code                                     AS SUB_DIST\n    , sub_district_name                                     AS SUB_DIST_DESC\n    , district_code                                         AS DIST \n    , district_name                                         AS DIST_DESC \n    , province_code                                         AS PROV \n    , province_name                                         AS PROV_NM\n    , post_code                                             AS POST_CD\nFROM\n    addr                                                    AS addr\nLEFT JOIN `data-prod-mobile.external.outbound_subdistrict`  AS sub\n    ON addr.subdistrict_code    = sub.subdistrict_id\nLEFT JOIN `data-prod-mobile.external.outbound_district`     AS dis\n    ON addr.district_code       = dis.district_id\n</code></pre> <p>Assume that this transform spec query use for ingest data to the target table on your warehouse name like <code>DWHMODEL.MODEL_HOUSE</code>.</p> <p>The data on the transform spec table will be like;</p> Database Table Name /File Name Column Name Data Type Key FieldDefinition Source System /Database Table Name /File Name Alias Column Business Rule Type Business Rule / Join / Condition Remark Updated Date Updated By Remark DWHMODEL MODEL_HOUSE - - - MOBILE data-prod-mobile.b2c_customer.transaction addr Filter SELECT DISTINCTcust_address, sub_district_code, district_code, province_code, post_codeFROM <code>data-prod-mobile.b2c_customer.transaction</code> DWHMODEL MODEL_HOUSE - - - MOBILE data-prod-mobile.external.outbound_subdistrict sub Join LEFT JOIN <code>data-prod-mobile.external.outbound_subdistrict</code> AS subON addr.subdistrict_code = sub.subdistrict_id DWHMODEL MODEL_HOUSE - - - MOBILE data-prod-mobile.external.outbound_district dis Join LEFT JOIN <code>data-prod-mobile.external.outbound_district</code> AS disON addr.district_code = dis.district_id DWHMODEL MODEL_HOUSE HOUSE_ID STRING Y House ident MOBILE data-prod-mobile.b2c_customer.transaction addr cust_addresssub_district_codedistrict_code province_codepost_code Business Rule md5(concat(  coalesce(cust_address, ''), coalesce(sub_district_code, ''), coalesce(district_code, ''), coalesce(province_code, ''), coalesce(post_code, ''))) Use <code>md5</code> algorithm DWHMODEL MODEL_HOUSE HOUSE_LVL INTEGER House level - N/A N/A N/A Constant 1 DWHMODEL MODEL_HOUSE ADDR STRING House's address MOBILE data-prod-mobile.b2c_customer.transaction addr cust_address Move DWHMODEL MODEL_HOUSE HOUSE_NO STRING House's Number - - - - Not Mapped DWHMODEL MODEL_HOUSE STREET STRING House's street - - - - Not Mapped DWHMODEL MODEL_HOUSE SUB_DIST STRING House's sub-district code MOBILE data-prod-mobile.b2c_customer.transaction addr sub_district_code Move DWHMODEL MODEL_HOUSE SUB_DIST_DESC STRING House's sub-district name MOBILE data-prod-mobile.external.outbound_subdistrict sub sub_district_name Move DWHMODEL MODEL_HOUSE DIST STRING House's district code MOBILE data-prod-mobile.b2c_customer.transaction addr district_code Move DWHMODEL MODEL_HOUSE DIST_DESC STRING House's district name MOBILE data-prod-mobile.external.outbound_district dis district_name Move DWHMODEL MODEL_HOUSE PROV STRING House's province code MOBILE data-prod-mobile.b2c_customer.transaction addr province_code Move DWHMODEL MODEL_HOUSE PROV_NM STRING House's province name MOBILE data-prod-mobile.b2c_customer.transaction addr province_name Move DWHMODEL MODEL_HOUSE POST_CD STRING House's post code MOBILE data-prod-mobile.b2c_customer.transaction addr post_code Move"},{"location":"metadata/control-framework/node-strategy/","title":"Node strategy","text":"<p>This concept use node to pass running date only.</p> <pre><code>core\n \u2570\u2500 node\n     \u251c\u2500 pipeline-label\n     \u2570\u2500 node-group-label\n</code></pre>"},{"location":"metadata/control-framework/node-strategy/metadata/","title":"Metadata","text":""},{"location":"metadata/control-framework/node-strategy/metadata/#schema-config","title":"Schema Config","text":""},{"location":"metadata/control-framework/node-strategy/metadata/#node","title":"Node","text":"Column Data Type PK Description node_name STRING Y Node process name node_group_name STRING priority INTEGER load_type STRING data_load_type STRING T, D, F, SCD1, SCD2_D, SCD2_F, SCD2_T extras JSON active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"metadata/control-framework/node-strategy/metadata/#node-dependency","title":"Node Dependency","text":"Column Data Type PK Description node_name STRING Y Node process name node_dependency_name STRING Y Node process dependency name dependency_set_date INTEGER A number for decrease dependency process date before running active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data <p>Note</p> <p>The <code>extras</code> field on the node table contain all of necessary node arguments;</p> <pre><code>extras\n  \u251c\u2500 files\n  \u2502   \u251c\u2500 name\n  \u2502   \u251c\u2500 path\n  \u2502   \u251c\u2500 header\n  \u2502   \u2570\u2500 encoding\n  \u251c\u2500 storage\n  \u2502   \u251c\u2500 system\n  \u2502   \u2570\u2500 container\n  \u2570\u2500 framwork\n      \u251c\u2500 archiving\n      \u2570\u2500 timeout\n</code></pre>"},{"location":"metadata/control-framework/node-strategy/metadata/#schema-data","title":"Schema Data","text":""},{"location":"metadata/control-framework/node-strategy/metadata/#node-logging","title":"Node Logging","text":"Column Data Type PK Description node_name STRING Y Node process name process_id INTEGER Y start_date DATETIME end_date DATETIME process_date DATETIME status STRING Success, Failed, Start log STRING records JSON update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data <p>Note</p> <p>The <code>records</code> field on the node log table contain all of necessary result records;</p> <pre><code>records\n  \u251c\u2500 src\n  \u2502   \u251c\u2500 count\n  \u2502   \u2570\u2500 control_count\n  \u2570\u2500 tgt\n      \u251c\u2500 count\n      \u2570\u2500 control_count\n</code></pre>"},{"location":"metadata/control-framework/pipeline-strategy/","title":"Pipeline strategy","text":"<p>This concept use pipeline to pass running date on all node processes.</p> <pre><code>core\n \u2570\u2500 pipeline\n     \u2570\u2500 node-group\n         \u2570\u2500 node\n</code></pre>"},{"location":"metadata/control-framework/pipeline-strategy/metadata/","title":"Metadata","text":""},{"location":"metadata/control-framework/pipeline-strategy/metadata/#schema-config","title":"Schema Config","text":""},{"location":"metadata/control-framework/pipeline-strategy/metadata/#pipeline","title":"Pipeline","text":"Column Data Type PK Description pipeline_name STRING Y Pipeline name set_date INTEGER A number for decrease process date before running frequency STRING 'D', 'W', 'M', 'Q', 'Y' data_frequency STRING 'D', 'W', 'M', 'Q', 'Y' active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"metadata/control-framework/pipeline-strategy/metadata/#node-group","title":"Node Group","text":"Column Data Type PK Description node_group_name STRING Y Node process group name pipeline_name STRING priority INTEGER active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"metadata/control-framework/pipeline-strategy/metadata/#node","title":"Node","text":"Column Data Type PK Description node_name STRING Y Node process name node_group_name STRING priority INTEGER load_type STRING data_load_type STRING T, D, F, SCD1, SCD2_D, SCD2_F, SCD2_T extras JSON active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"metadata/control-framework/pipeline-strategy/metadata/#node-dependency","title":"Node Dependency","text":"Column Data Type PK Description node_name STRING Y Node process name node_dependency_name STRING Y Node process dependency name dependency_set_date INTEGER A number for decrease dependency process date before running active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data <p>Note</p> <p>The <code>extras</code> field on the node table contain all of necessary node arguments;</p> <pre><code>extras\n  \u251c\u2500 files\n  \u2502   \u251c\u2500 name\n  \u2502   \u251c\u2500 path\n  \u2502   \u251c\u2500 header\n  \u2502   \u2570\u2500 encoding\n  \u251c\u2500 storage\n  \u2502   \u251c\u2500 system\n  \u2502   \u2570\u2500 container\n  \u2570\u2500 framwork\n      \u251c\u2500 archiving\n      \u2570\u2500 timeout\n</code></pre>"},{"location":"metadata/control-framework/pipeline-strategy/metadata/#schema-data","title":"Schema Data","text":""},{"location":"metadata/control-framework/pipeline-strategy/metadata/#pipeline-watermark-merge","title":"Pipeline Watermark (Merge)","text":"Column Data Type PK Description pipeline_name STRING Y Pipeline name process_date DATETIME Process datetime that be the latest process running next_process_date DATETIME previous_process_date DATETIME data_date DATETIME next_data_date DATETIME previous_data_date DATETIME running_flag BOOLEAN A running flag for tacking this pipeline is running or not update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"metadata/control-framework/pipeline-strategy/metadata/#pipeline-logging-append-only","title":"Pipeline Logging (Append Only)","text":"Column Data Type PK Description pipeline_name STRING Y Pipeline name process_id INTEGER Y Process ID orchestrate_id INTEGER Y Orchestration tools running id process_ts TIMESTAMP process_date DATETIME status STRING 'Success', 'Failed', 'Start' log STRING tracking JSON update_stage STRING update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"metadata/control-framework/pipeline-strategy/metadata/#node-logging-append-only","title":"Node Logging (Append Only)","text":"Column Data Type PK Description node_name STRING Y Node process name process_id INTEGER Y Process ID orchestrate_id INTEGER Y Orchestration tools running id process_ts TIMESTAMP Process timestamp process_date DATETIME Process datetime status STRING 'Success', 'Failed', 'Start' log STRING records JSON update_stage STRING update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data <p>Note</p> <p>The <code>records</code> field on the node log table contain all of necessary result records;</p> <pre><code>records\n  \u251c\u2500 count\n  \u2570\u2500 control_count\n</code></pre> <p>If you get the log <code>records</code> that not null and grouping its by <code>update_stage</code>, it will be tracing record like;</p> <pre><code>records\n  \u251c\u2500 src\n  \u2502   \u251c\u2500 count\n  \u2502   \u2570\u2500 control_count\n  \u251c\u2500 stg ...\n  \u251c\u2500 stg ...\n  \u2570\u2500 tgt\n      \u251c\u2500 count\n      \u2570\u2500 control_count\n</code></pre>"},{"location":"metadata/control-framework/pipeline-strategy/metadata/#query","title":"Query","text":""},{"location":"metadata/control-framework/pipeline-strategy/metadata/#node-information","title":"Node information","text":"<pre><code>SELECT\n      pipe.pipeline_name                    AS pipeline_name\n    , node_group.node_group_name            AS node_group_name\n    , node.*\nFROM control.control_pipeline               AS pipe\nJOIN control.control_node_group             AS node_group\n    ON  pipe.pipeline_name   = node_group.node_group_name\nJOIN control.control_node                   AS node\n    ON  node_group.node_name = node.node_name\nWHERE\n        pipe.active_flag        is true\n    AND node_group.active_flag  is true\n    AND node.active_flag        is true\n</code></pre>"},{"location":"metadata/control-framework/pipeline-strategy/metadata/#node-log","title":"Node log","text":"<pre><code>SELECT\n    node_name\n    , process_id\n    , orchestrate_id\n    , MIN(process_date)                     AS start_date\n    , MAX(process_date)                     AS end_date\nFROM control.log_node\nGROUP BY\n    node_name\n    , process_id\n    , orchestrate_id\n</code></pre>"},{"location":"metadata/control-framework/pipeline-strategy/metadata/#node-dependency-needed","title":"Node dependency needed","text":"<pre><code>SELECT\n    node_deps.node_dependency_name          AS node_deps_name\n    , CASE WHEN MAX(COALESCE(log.process_id, -999)) = -999 THEN 'Need'\n           ELSE 'Pass'\n      END                                   AS latest_process_id\nFROM control.control_node                   AS node\nINNER JOIN control.control_node_dependency  AS node_deps\n    ON node.node_name   = node_deps.node_name\nLEFT JOIN control.log_node                  AS log\n    ON log.node_name    = node_deps.node_dependency_name\nWHERE\n        log.status       = 'Success'\n    AND '{process-date}' = DATEADD(DAY, node_deps.dependency_set_date, log.process_date)\nGROUP BY node_deps.node_dependency_name\n</code></pre>"},{"location":"metadata/domains/address/","title":"Address","text":""},{"location":"metadata/domains/customer/","title":"Customer Profile","text":""},{"location":"metadata/domains/customer/#profile","title":"Profile","text":"Field Name Alias Data Type PK Description <code>customer_id</code> cust_id STRING Y Unique identifier for the customer <code>first_name</code> fname STRING Customer's first name <code>last_name</code> lname STRING Customer's last name <code>email</code> email STRING Customer's email address <code>phone_number</code> phone STRING Customer's phone number <code>dob</code> dob DATE Date of birth <code>gender</code> gender STRING Customer's gender (e.g., \"M\", \"F\", \"Other\") <code>address</code> address STRING Full address (e.g., street, city, state, zip) <code>register_date</code> register DATE Date the customer signed up <code>email_opt_in</code> BOOLEAN Whether the customer opted in for email communications <code>sms_opt_in</code> BOOLEAN Whether the customer opted in for SMS communications"},{"location":"metadata/domains/customer/#family","title":"Family","text":"Field Name Alias Data Type PK Description <code>family_id</code> fam_id STRING Y Unique identifier for the family <code>family_name</code> fam_name STRING Family's name (e.g. last name of customer) <code>customer_id</code> cust_id STRING Identifier for the customer that stay on this family"},{"location":"metadata/structures/etl/","title":"ETL","text":"<p>The table that use the ETL process that ingest data to it should has process tracking field for tracking this record ingest from which process.</p> Field Name Alias Data Type PK Description process_name ps_nm STRING process_date ps_date DATETIME update_process_name update_ps_nm STRING update_process_date update_ps_date DATETIME"},{"location":"metadata/structures/etl/#delete-before","title":"Delete before","text":"<pre><code>DELECT FROM {target-schema}.{target-table-name}\nWHERE\n        process_name =  '{process-name}'\n    AND process_date &gt;= '{current-date}'\n</code></pre>"},{"location":"metadata/structures/master/","title":"Master","text":"Field Name Alias Data Type PK Description created_at created DATETIME Timestamp when the data was created updated_at updated DATETIME Timestamp when the data was last updated"},{"location":"metadata/structures/master/#scd1-full-dump","title":"SCD1 (Full-Dump)","text":"Field Name Alias Data Type PK Description id id STRING Y Surrogate key that unique per row of data business_id buz_id STRING deleted_flag deleted_f BOOLEAN Delete flag that the data does not available"},{"location":"metadata/structures/master/#scd2","title":"SCD2","text":"Field Name Alias Data Type PK Description id id STRING Y Surrogate key that unique per row of data business_id buz_id STRING Business key that is the unique key when you fiter active_flag equal true active_flag active_f BOOLEAN Active flag that the data is the current update start_date valid_start DATETIME Timestamp when the data start to use end_date valid_end DATETIME Timestamp when the data end to use"},{"location":"metadata/structures/transaction/","title":"Transaction","text":"Field Name Alias Data Type PK Description created_at created TIMESTAMP Timestamp when the transaction was created"}]}